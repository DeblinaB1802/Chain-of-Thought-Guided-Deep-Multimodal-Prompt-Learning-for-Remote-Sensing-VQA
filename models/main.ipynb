{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5d38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RSVQADatasetClip] samples=77232 | skipped inactive=0, no_img_id=0, no_answers=0, bad_types=0, unmapped_to_9=0, missing_img=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\venv\\Lib\\site-packages\\torch\\_compile.py:53: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n",
      "Epoch 1/10 [Train]:   0%|          | 0/2173 [00:00<?, ?it/s]c:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/10 [Train]:   0%|          | 0/2173 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (55) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m images_subdir = \u001b[33m\"\u001b[39m\u001b[33mImages_LR/Images_LR\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Training configuration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages_subdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_subdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_answers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_vprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_ckpt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoint_best.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Evaluation configuration\u001b[39;00m\n\u001b[32m     32\u001b[39m     evaluate(\n\u001b[32m     33\u001b[39m         data_root=data_root,\n\u001b[32m     34\u001b[39m         checkpoint_path=\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_best.pth\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# update if different\u001b[39;00m\n\u001b[32m     35\u001b[39m         images_subdir=images_subdir,\n\u001b[32m     36\u001b[39m         batch_size=\u001b[32m32\u001b[39m\n\u001b[32m     37\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\models\\train.py:119\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(data_root, images_subdir, num_epochs, batch_size, lr, max_answers, prompt_len, prompt_depth, num_vprompts, val_ratio, seed, out_ckpt)\u001b[39m\n\u001b[32m    116\u001b[39m     img_feats_aux = clip_model.encode_image(images)\n\u001b[32m    118\u001b[39m text_feats, vprompts_list, _ = text_prompt(img_feats_aux, tokenized)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m image_feats = \u001b[43mvisual_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvprompts_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m logits = fusion_head(text_feats, image_feats)\n\u001b[32m    122\u001b[39m loss = criterion(logits, acls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\models\\visual_prompt_learner.py:47\u001b[39m, in \u001b[36mVisualPromptLearner.forward\u001b[39m\u001b[34m(self, images, vprompts_list)\u001b[39m\n\u001b[32m     44\u001b[39m concat = torch.cat([\u001b[38;5;28mcls\u001b[39m, vprompts, rest], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m pos = \u001b[38;5;28mself\u001b[39m.clip_visual.positional_embedding[:concat.size(\u001b[32m1\u001b[39m), :].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m concat = \u001b[43mconcat\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\n\u001b[32m     48\u001b[39m x = block(concat)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# drop prompts before passing to next layer\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (55) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "from train import train\n",
    "from evaluate import evaluate\n",
    "\n",
    "\n",
    "# === Choose mode: \"train\" or \"eval\" ===\n",
    "mode = \"train\"  # change to \"eval\" for evaluation\n",
    "\n",
    "# === Common paths ===\n",
    "data_root = r\"C:\\Users\\debli\\OneDrive\\Desktop\\MTP_implementation\\data\"  # update this path\n",
    "images_subdir = \"Images_LR/Images_LR\"\n",
    "\n",
    "if mode == \"train\":\n",
    "    # Training configuration\n",
    "    train(\n",
    "        data_root=data_root,\n",
    "        images_subdir=images_subdir,\n",
    "        num_epochs=10,\n",
    "        batch_size=32,\n",
    "        lr=1e-4,\n",
    "        max_answers=9,\n",
    "        prompt_len=4,\n",
    "        prompt_depth=5,\n",
    "        num_vprompts=5,\n",
    "        val_ratio=0.1,\n",
    "        seed=42,\n",
    "        out_ckpt=\"checkpoint_best.pth\"\n",
    "    )\n",
    "\n",
    "elif mode == \"eval\":\n",
    "    # Evaluation configuration\n",
    "    evaluate(\n",
    "        data_root=data_root,\n",
    "        checkpoint_path=\"checkpoint_best.pth\",  # update if different\n",
    "        images_subdir=images_subdir,\n",
    "        batch_size=32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d13572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ece6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
